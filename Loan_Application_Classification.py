# -*- coding: utf-8 -*-
"""Loan_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jSuxGE1pX-N04V-gab6MmNuFXiQjlpJ0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/lending_club_loan_two.csv")

df.T

df.info()

df.describe()

df.isnull().sum()



sns.histplot(x="loan_amnt", data=df, bins=30)

df.corr()

plt.figure(figsize=(20,15))
sns.heatmap(df.corr(), cmap="coolwarm", annot=True)

sns.scatterplot(x="installment", y="loan_amnt", data=df)

sns.boxplot(x="loan_status", y="loan_amnt", data=df)

df.head()

df.groupby("loan_status")["loan_amnt"].describe()

print(df["grade"].unique())

print(df["sub_grade"].unique())

sns.countplot(x="grade", data=df, hue="loan_status")

plt.figure(figsize=(18,15))
subgrade_order = sorted(df["sub_grade"].unique())
sns.countplot(x="sub_grade", data=df, hue="loan_status",palette="coolwarm", order=subgrade_order)

sorted(df["sub_grade"].unique())

plt.figure(figsize=(10,6))
f_g = df[(df["grade"] == "G") | (df["grade"] == "F")]
subgrade_order = sorted(f_g["sub_grade"].unique())
sns.countplot(x="sub_grade", data=f_g, order=subgrade_order, hue="loan_status")

df["loan_repaid"] = df["loan_status"].map({"Fully Paid":1, "Charged Off": 0})

df[["loan_status","loan_repaid"]]

df.corr()["loan_repaid"].sort_values().drop("loan_repaid").plot(kind="bar")

df.T

len(df)

df.isnull().sum()

df.isnull().sum() / len(df) * 100

df["emp_title"].nunique()

df["emp_title"].value_counts()

df.drop("emp_title", axis=1, inplace=True)

df

sorted(df["emp_length"].dropna().unique())

emp_length_order = [
   '< 1 year'
  ,'1 year',
 '2 years',
 '3 years',
 '4 years',
 '5 years',
 '6 years',
 '7 years',
 '8 years',
 '9 years',
 '10+ years',
]

emp_length_order

plt.figure(figsize=(10,8))
sns.countplot(x="emp_length", data=df, order=emp_length_order, hue="loan_status")

emp_co = df[df["loan_status"] == "Charged Off"].groupby("emp_length").count()["loan_status"]

emp_fp = df[df["loan_status"] == "Fully Paid"].groupby("emp_length").count()["loan_status"]

(emp_co / emp_fp).plot(kind="bar")

df.drop("emp_length", axis=1, inplace=True)

df

df.isnull().sum()

df["purpose"]

df["title"]

df.drop("title", axis=1, inplace=True)

df.isnull().sum()

df["mort_acc"].value_counts()

df.corr()["mort_acc"].sort_values()

total_acc_avg = df.groupby("total_acc").mean()["mort_acc"]
total_acc_avg

def fill_mort_acc(total_acc, mort_acc):
  
  if np.isnan(mort_acc):
    return total_acc_avg[total_acc]
  else:
    return mort_acc

df["mort_acc"] = df.apply(lambda x: fill_mort_acc(x["total_acc"], x["mort_acc"]), axis=1)

df.isnull().sum()

"""## Categorical Varibales"""

df.select_dtypes(["object"]).columns

df["term"]

df["term"].value_counts()

df["term"] = df["term"].apply(lambda term: int(term[:3]))

df["term"]

df.select_dtypes(["object"]).columns

df["grade"]

df.drop("grade", axis=1, inplace=True)

df.select_dtypes(["object"]).columns

dummies = pd.get_dummies(df["sub_grade"], drop_first=True)

df = pd.concat([df.drop("sub_grade", axis=1), dummies], axis=1)

df.T

df.columns

df.isnull().sum()

dummies = pd.get_dummies(df["verification_status"], drop_first=True)

df = pd.concat([df.drop("verification_status", axis=1), dummies], axis=1)

df.columns

dummies = pd.get_dummies(df["application_type"], drop_first=True)

df = pd.concat([df.drop("application_type", axis=1), dummies], axis=1)

df.columns

dummies = pd.get_dummies(df["initial_list_status"], drop_first=True)

df = pd.concat([df.drop("initial_list_status", axis=1), dummies], axis=1)

df.columns

dummies = pd.get_dummies(df["purpose"], drop_first=True)

df = pd.concat([df.drop("purpose", axis=1), dummies], axis=1)

df.columns

df.isnull().sum()

df["home_ownership"].value_counts()

df["home_ownership"].replace(["ANY", "NONE"], "OTHER", inplace=True)

df["home_ownership"].value_counts()

dummies = pd.get_dummies(df["home_ownership"], drop_first=True)

df = pd.concat([df.drop("home_ownership", axis=1), dummies], axis=1)

df.columns

df["address"]

df["address"].value_counts()

df["zip_code"] = df["address"].apply(lambda address: address[-5:])

df.columns

df["zip_code"].value_counts()

dummies = pd.get_dummies(df["zip_code"], drop_first=True)

df = pd.concat([df.drop("zip_code", axis=1), dummies], axis=1)

df.columns

df.drop("address", axis=1, inplace=True)

df["issue_d"]

df.drop("issue_d", axis=1, inplace=True)

df["earliest_cr_line"]

df["earliest_cr_year"] = pd.DatetimeIndex(df["earliest_cr_line"]).year

df["earliest_cr_year"]

df.drop("earliest_cr_line", axis=1, inplace=True)

df.columns

from sklearn.model_selection import train_test_split

df["loan_status"]

df["loan_repaid"]

df.drop("loan_status", axis=1, inplace=True)

df.isnull().sum()

from sklearn.model_selection import train_test_split

X = df.drop("loan_repaid", axis=1)
y = df["loan_repaid"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

"""# Creating the Model"""

len(df)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()

# input layer
model.add(Dense(78,  activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(39, activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(19, activation='relu'))
model.add(Dropout(0.2))

# output layer
model.add(Dense(units=1,activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam')



df.isnull().sum()

df

df[df["revol_util"] != 0.0]

df = df.dropna()

X = df.drop("loan_repaid", axis=1)
y = df["loan_repaid"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()

model.add(Dense(78,activation="relu"))
model.add(Dropout(0.2))

model.add(Dense(38,activation="relu"))
model.add(Dropout(0.2))

model.add(Dense(19,activation="relu"))
model.add(Dropout(0.2))

model.add(Dense(1,activation="sigmoid"))

model.compile(loss="binary_crossentropy", optimizer="adam")

model.fit(X_train, y_train, epochs=30, batch_size=256, validation_data=(X_test, y_test))

from tensorflow.keras.models import load_model

model.save("mymodel.h5")



"""# Evaluating Model"""

model.history.history

losses = pd.DataFrame(model.history.history)

losses

losses.plot()

from sklearn.metrics import classification_report, confusion_matrix

y_preds = (model.predict(X_test) > 0.5) * 1

print(classification_report(y_test, y_preds))
print("\n")
print(confusion_matrix(y_test, y_preds))

import random
random.seed(101)
random_index = random.randint(0,len(df))

new_customer = df.drop("loan_repaid", axis=1).iloc[random_index]

new_customer

new_customer = scaler.transform(new_customer.values.reshape(1,78))

np.argmax(model.predict(new_customer), axis=1)

from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV

gradient_model = HistGradientBoostingClassifier()
lr_model = LogisticRegression(max_iter=10000)

gradient_model.fit(X_train, y_train)

y_preds = gradient_model.predict(X_test)

print(classification_report(y_test, y_preds))

df

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression

df.dropna(inplace=True)

X = df.drop("loan_repaid", axis=1)
y = df["loan_repaid"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

rf_model = RandomForestClassifier()
lr_model = LogisticRegression(max_iter=10000)

rf_model.fit(X_train, y_train)
lr_model.fit(X_train, y_train)

y_preds = rf_model.predict(X_test)

print(classification_report(y_test, y_preds))

y_preds_lr = lr_model.predict(X_test)

print(classification_report(y_test, y_preds_lr))
RandomForestClassifier()

params_grid = {"n_estimators": [100,200,400],
               "min_samples_leaf": [1,2,4],
               "min_samples_split": [2,4,6],
               "max_features": ["sqrt", "auto"]}

rs_model = RandomizedSearchCV(estimator=RandomForestClassifier(),
                              param_distributions=params_grid,
                              verbose=2,cv=3,n_jobs=2, n_iter=5)

rs_model.fit(X_train, y_train)

y_preds = rs_model.predict(X_test)

